{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Techniques\n",
    "\n",
    "_Kevin Siswandi_  \n",
    "**Fundamentals of machine learning**  \n",
    "June 2020  \n",
    "\n",
    "- Many parameters need to be selected\n",
    "- The fidelity of the low-dimensional representation depends on the chosen parameters and intrinsic dimensionality of the data.\n",
    "- The parameters control the tradeoff between stiffness and flexibility\n",
    "\n",
    "We say that the lower-dimensional projection is either too stiff or too flexible when\n",
    "- Too stiff = faraway points get mapped to nearby locations\n",
    "- Too flexible = nearby points get mapped to faraway locations\n",
    "\n",
    "In real-world datasets, usually the intrinsic dimensionality is much lower than the nominal dimensionality. The first thing to do, is almost always PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "We will approach PCA here from the perspective of Singular Value Decomposition. We are looking for a rank-1 approximation to find the principal components. This is achived by decomposing the original data matrix X ($p \\times n$) into:\n",
    "- principal components -- new basis in terms of old basis\n",
    "- scores -- coordinates in the new, lower dimensional basis\n",
    "\n",
    "Some properties:\n",
    "- The principal component matrix $U$ is of shape $p \\times r$, where $r \\leq p$.\n",
    "- The scores matrix $Z$ is $r \\times n$.\n",
    "- The principal components are orthonormal to each other, i.e. $U^T U = I$\n",
    "\n",
    "Singular Value Decomposition says that X can always be decomposed into\n",
    "\n",
    "$$ X = U Z = U S V^T$$\n",
    "\n",
    "where S is a diagonal matrix. The optimization problem solved by PCA is\n",
    "\n",
    "$$ arg\\min_{U, Z} || X - UZ||_F^2 $$\n",
    "\n",
    "such that the product UZ has rank r. The decomposition is unique if singular values in diagonal matrix S are unique (nondegenerate). As a concrete example, consider a data matrix X of shape $p \\times n$, where $p \\leq n$. Then:\n",
    "* U is of shape p x p\n",
    "* S is of shape p x p\n",
    "* $V^T$ is of shape p x n\n",
    "* $U^T U = I_p$ (identity matrix of p dimension)\n",
    "* $V V^T = I_n$ (identity matrix of n dimension)\n",
    "\n",
    "The optimization problem can also be written as\n",
    "\n",
    "$$ arg\\min_\\tilde{X} || X - \\tilde{X}||_F^2 $$\n",
    "\n",
    "such that rank($\\tilde{X}$) = r, where:\n",
    "* X -- original data\n",
    "* Z -- low dimensional representation (encoding of X)\n",
    "* $\\tilde{X}$ -- approximation to original data/decoding of Z\n",
    "\n",
    "Note that the encoding can be done via $ Z = U^T X$, and the approximation/decoding computed as $ \\tilde{X} = UZ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "Note that both the encoding and decoding in PCA are linear operations. How do we go nonlinear? In autoencoders, the encoding and decoding are implemented by nonlinear neural networks. This is achieved by:\n",
    "\n",
    "$$ arg\\min_{\\theta_e, \\theta_d} \\sum_i L(x_i, f_d(f_e(x_i; \\theta_e); \\theta_d)) $$\n",
    "\n",
    "where L is the loss function, $f_e$ and $f_d$ are the encoding and decoding functions (from the neural network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other techniques\n",
    "\n",
    "Here we review some methods that make high- and low- dimensional dissimilarities match.\n",
    "\n",
    "### Metric Multidimensional Scaling\n",
    "* Dissimilarity measure in high-dimensional space: Euclidean distance $d_{ij}^* = || x_i - x_j ||_2$\n",
    "* Dissimilariy measure in low-dimensional space: Euclidean distance $d_{ij} = || z_i - z_j ||_2$\n",
    "* Optimization objective: $arg\\min_z \\sum_i \\sum_j (d_{ij}^* - d_{ij})^2$\n",
    "\n",
    "The drawbacks of this method are 2fold:\n",
    "- It emphasizes long distances too much\n",
    "- non-convex optimization problem\n",
    "\n",
    "There are also variants of this, such as weighting the optimization objective by the distances (either in low- or high- dimensional space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvilinear Component Analysis\n",
    "\n",
    "This method [Demartinez, 1996] extends the MDS method by weighting the optimization objective by the low-dimensional distances. This allows for stronger or more non-linear unfolding of manifolds.\n",
    "\n",
    "### Stochastic Neighbour Embedding\n",
    "\n",
    "In this method [Hinton, 2003], the similarity is measured using radial basis function (RBF):\n",
    "\n",
    "$$ k^*(x_i, x_j) = \\exp(-\\kappa_i ||x_i - x_j||^2) $$\n",
    "\n",
    "where the constant $\\kappa_i$ can be different for each observation. The similarity is also measured using RBF in the low-dimensional space, and the optimization uses the KL divergence between the two similarities.\n",
    "\n",
    "###  t-SNE\n",
    "\n",
    "t-SNE [van der Maaten, 2008] uses RBF kernel in the high-dimensional space, but student-t kernel in the lower dimensional space:\n",
    "\n",
    "$$ k(z_i, z_j) = \\frac{1}{1 + \\alpha ||z_i - z_j||_2^2} $$\n",
    "\n",
    "However, this is still a hard optimization problem. Depending on the initialization, you can end up in different local minima. A workaround is to initialize with a deterministic procedure (e.g. results from PCA). Computationally, this is also expensive because we minimize over all coordinates in the low-dimensional space: If we have 1 million points, there are two million coordinates! (since 1 observation has two coordinates).\n",
    "\n",
    "### UMAP\n",
    "\n",
    "Uniform Manifold Approximation uses similarity kernel that is constant up to nearest neighbour (and RBF otherwise) in the high-dimensional space. In the low-dimensional space, the single-sided exponential kernel is used as the similarity kernel. The optimization problem is minimizing the cross entropy between the two similarities, and initialization is done using Laplacian eigenmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
