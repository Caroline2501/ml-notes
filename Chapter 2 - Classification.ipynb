{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "_Kevin Siswandi_  \n",
    "**Fundamentals of Machine Learning**  \n",
    "7 June 2020  \n",
    "\n",
    "In supervised learning, we are given training data $\\{x_i, y_i\\}$, where the features/explanatory variables/attributes $x_i$ can be either:\n",
    "+ nominal -- categories (e.g. blue, green, red)\n",
    "+ ordinal -- categories with ordering (e.g. cold, hot)\n",
    "+ interval -- differences (e.g. centigrade scale)\n",
    "+ ratio -- interval with absolute/meaningful zero (e.g. Kelvin scale)\n",
    "\n",
    "and the $y_i$ is called labels (for classification task) or targets/outputs (for regression task). What we want out of a supervised learning task is to extract rules that can make accurate predictions of y given x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-nearest neighbour classification\n",
    "\n",
    "Here it is conceptually equivalent to constructing a Voronoi diagram, where each Voronoi region belongs to a single instance. Therefore, the label of a test instance can be predicted based on the Voronoi region it is located in. However, storing the Voronoi regions is expensive in multi-dimensional space. Similarly, computing the decision boundary for the Voronoi diagram is also not efficient. Instead, 1NN is often implemented by finding the nearest neighbor of a query and predict the label based on the label of its nearest neighbour.\n",
    "\n",
    "So in 1NN, we predict the class membership of a query based on the label of its nearest neighbour (axis scaling matters). One disadvantage of 1NN is that as we go to higher dimensions, practically all observations contribute to the decision boundary, creating artifacts such as 'islands'. Further properties:\n",
    "- low bias: low error on the training set (averaging over multiple classifiers trained on dataset generated from the same distribution)\n",
    "- high variance: the decisions boundary will be very different (averaging over multiple classifiers trained on dataset generated from the same distribution)\n",
    "- lazy strategy: zero compute at train time\n",
    "- $\\mathcal{O}(n)$ effort for each prediction, to find the nearest neighbour\n",
    "\n",
    "To reduce variance, we can take the **k nearest neighbours**. Typically k is odd number. However, for large k, we incur a larger bias. Overall, k-NN is a nonparametric method (c.f. linear regression which is a parametric method that has a fixed number of parameters) because the complexity of the classifier increases with the size of training data.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/1920px-Euclidean_Voronoi_diagram.svg.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "_Image: 1st order Voronoi diagram with Euclidean distance (from Wikipedia)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Cross validation is the universal technique for choosing hyperparameters. However, if you are rich in training data, you can simply partition your data randomly as (e.g. 70% - 20% - 10%):\n",
    "\n",
    "![train-val-test split](https://miro.medium.com/max/1400/1*Nv2NNALuokZEcV6hYEHdGA.png)\n",
    "\n",
    "Then iterate:\n",
    "* adjust hyperparameters\n",
    "* train classifier on training data\n",
    "* evaluate classifier on validation data\n",
    "\n",
    "until the accuracy on validation set is acceptable. Finally, evaluate accuracy of the best-effort classifier on test data (only once! otherwise it will no longer give a valid estimate of the generalization error). The classifier usually works more reliably with more training data. Hence, retrain your classifier on the entire data (training + validation + test) and deploy.\n",
    "\n",
    "Often, we are not rich in training data (< 10,000), so we need to use **cross-validation** which works as follows.\n",
    "+ partition the data randomly into k folds, typically k = 10.\n",
    "+ iterate (until satisfied):\n",
    "    - adust hyperparameters\n",
    "    - for 1 to k do:\n",
    "        * train on all but one fold\n",
    "        * evaluate accuracy on held-out fold\n",
    "    - estimate the mean and variance (uncertainty estimate) of accuracy\n",
    "+ retrain classifier on entire data and deploy\n",
    "\n",
    "Cross-validation is an empirical estimate, but there are also theoretical estimates for how much overfitting a choice of hyperparameters would give. However, these theoretical estimates make assumptions about the model and the data, which usually do not hold in practice, such as Akaike Information Criterion, Bayesian Information Criterion, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
