{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "In supervised learning, we are given training data $\\{x_i, y_i\\}$, where the features/explanatory variables/attributes $x_i$ can be either:\n",
    "+ nominal -- set membership (e.g. has windows, doors)\n",
    "+ ordinal -- allows ordering (e.g. cold, hot)\n",
    "+ interval -- measures differences (e.g. centigrade scale)\n",
    "+ ratio -- interval with absolute/meaningful zero (e.g. Kelvin scale)\n",
    "\n",
    "and the $y_i$ is called labels (for classification task) or targets/outputs (for regression task). What we want out of a supervised learning task is to extract rules that can make accurate predictions of y given x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-nearest neighbour classification\n",
    "\n",
    "Here it is conceptually equivalent to constructing a Voronoi diagram, where each Voronoi region belongs to a single instance. Therefore, the label of a test instance can be predicted based on the Voronoi region it is located in. However, storing the Voronoi regions is expensive in multi-dimensional space. Similarly, computing the decision boundary for the Voronoi diagram is also not efficient. Instead, 1NN is often implemented by finding the nearest neighbor of a query and predict the label based on the label of its nearest neighbour.\n",
    "\n",
    "So in 1NN, we predict the class membership of a query based on the label of its nearest neighbour (axis scaling matters). One disadvantage of 1NN is that as we go to higher dimensions, practically all observations contribute to the decision boundary, creating artifacts such as 'islands'. Further properties:\n",
    "- low bias: low error on the training set (averaging over multiple classifiers trained on dataset generated from the same distribution)\n",
    "- high variance: the decisions boundary will be very different (averaging over multiple classifiers trained on dataset generated from the same distribution)\n",
    "- lazy strategy: zero compute at train time\n",
    "- $\\mathcal{O}(n)$ effort for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
