{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "_Kevin Siswandi_  \n",
    "**Fundamentals of Machine Learning**  \n",
    "June 2020\n",
    "\n",
    "The goal of cluster analysis is to find groups (representative examples) in data. Examples:\n",
    "* Mean shift: finding modes based on density estimate\n",
    "* KMeans\n",
    "* DBSCAN\n",
    "\n",
    "There are many ways to classify clustering methods:\n",
    "* Hierarchical\n",
    "    - agglomerative (single-linkage, complete linkage, etc.)\n",
    "    - divisive -- usually less computationally efficient than agglomerative\n",
    "* Flat\n",
    "    - crisp (e.g. k-means, mean-shift)\n",
    "    - fuzzy (e.g. Gaussian Mixture Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "With hierarchical clustering, we can see at which level clusters merge via a dendogram. Three distance criteria are possible:\n",
    "1. Single linkage -- shortest distance between any two points in two clusters\n",
    "2. Complete linkage -- \n",
    "3. Average linkage\n",
    "\n",
    "which usually give different results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# agglomerative clustering\n",
    "Iteratively merge 'closest' clusters\n",
    "    update distance of new cluster to all others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **single linkage clustering**, distance between cluster 1 and cluster 2 is found by\n",
    "\n",
    "$$ \\min_{i \\in C_1, j \\in C_2} d_{i,j} $$\n",
    "\n",
    "where $d_{ij}$ can be Euclidean, Mahalanobis, etc. This is equivalent to building a minimum spanning tree and truncates it:\n",
    "- Spanning (a subgraph of a full graph, where each node is connected to any other through the subgraph)\n",
    "- Minimum (the sum of edge weights used to construct the subgraph is minimal)\n",
    "- Tree (no cycles)\n",
    "\n",
    "Truncating is done by eliminating some edges that are above some threshold. This gives the same result as the dendogram approach but can be solved efficiently with Prim's or Kruskal's algorithm. The limitation of single linkage, however, is that noise can 'bridge' or 'chain' clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **complete linkage clustering**, it works the same but now we use\n",
    "\n",
    "$$ \\max_{i \\in C_1, j \\in C_2} d_{i,j} $$\n",
    "\n",
    "The limitation, however, is that it does not allow elongated clusters (produces compact clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we have the **Lance-Williams formulation for agglomerative clustering**. The pseudocode is:\n",
    "\n",
    "```\n",
    "init d_ij = || x_i - x_j || # L2 norm, with some distance criteria\n",
    "\n",
    "Repeat {\n",
    "    (i, j) = arg min d_ij\n",
    "    create new cluster (i, j)\n",
    "    delete cluster i and cluster j\n",
    "    for every cluster k, update distance to new cluster (i,j):\n",
    "        d_k(i,j) =  a1 * d_ki + a2 * d_kj + b * d_ij + g * abs(d_kj - d_ki)\n",
    "}\n",
    "```\n",
    "\n",
    "With this formulation, all standard linkage criteria can be applied by choosing the appropriate coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "Given a parameter k:\n",
    "1. Find coarse density estimate by computing, for each point $i$, the distance to the k-th nearest neighbor $d_i^k$ -- called core distance.\n",
    "2. Points with $d_i^k < \\theta$, where $\\theta$ is some threshold, are high density points -- called core points.\n",
    "3. Find the connected components of core points\n",
    "4. Assign nearby non-core points to clusters\n",
    "\n",
    "This algorithm works well for clusters of similar density even in the presence of noise. The drawback is that the parameters make a lot of difference: changing the parameters will give very different clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
