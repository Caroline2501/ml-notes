{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The regression problem\n",
    "\n",
    "_Kevin Siswandi_  \n",
    "Fundamentals of Machine Learning  \n",
    "June 2020\n",
    "\n",
    "In regression, one predicts a response/dependent variable from known explanatory variables/independent variables/features/responses. Some applications include:\n",
    "* interpolation/extrapolation/prediction/estimation of the response\n",
    "* parameter estimation (including uncertainty)\n",
    "* interpretation, e.g. determination the most influential explanatory variables\n",
    "\n",
    "Linear regression is linear in the parameters, but it can be nonlinear in explanatory variables. As an example, when measurements with quadratic behavior are fitted with a straight line, we find that **the residuals are highly correlated**. In this case, we can create a second feature $x^2$ and fit a linear hyperplane to the features $(x, x^2)$ with coefficients $\\beta_1, \\beta_2$.\n",
    "\n",
    "<img src=\"img/quadratic-regression.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and assumptions for linear regression\n",
    "\n",
    "The linear regression model is given by:\n",
    "\n",
    "$$ y = \\beta^T x + \\epsilon(x) $$\n",
    "\n",
    "where $x$ is deterministic (i.e. given) and exact (no measurement errors in explanatory variables), $\\beta$ is the (true) parameter that is deterministic, $\\epsilon(x)$ is random noise, and as a result $y$ is also a random variable. It should be noted that the residuals of LR/OLS (ordinary least squares) are evaluated only in $y$; c.f. residuals of TLS (total least squares) that minimize the perpendicular distance to the fitted line, which is used in PCA. The assumptions are:\n",
    "1. The explanatory variables are exact\n",
    "2. $\\mathbb{E}_\\epsilon[\\epsilon(x)] = 0$\n",
    "3. $\\mathrm{Var}[\\epsilon(x)] = \\mathrm{const}$, independent of x (homoscedasticity)\n",
    "4. $\\epsilon(x)$ are iid (independent and identically distributed)\n",
    "\n",
    "Now we collect given data into matrices: $Y$ with shape $(1 \\times n)$, $X$ with shape $(p \\times n)$, $\\epsilon$ with shape $(1 \\times n)$, and $\\beta^T$ is $(1 \\times p)$. We want to find parameters $\\beta$ that minimize the sum of squared residuals (SSQ) defined as:\n",
    "\n",
    "$$ \\mathrm{SSQ} = (Y - \\beta^T X) (Y - \\beta^T X)^T $$\n",
    "\n",
    "where $Y$ is the vector of measured responses, the SSQ is a scalar number and the two terms have shapes $1 \\times n$ and $n \\times 1$ respectively. We therefore take the derivative of the SSQ w.r.t. the parameters:\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{SSQ}}{\\partial \\beta} = -2 Y X^T + 2 \\beta^T X X^T $$\n",
    "\n",
    "Setting this to zero, we get the so-called **normal equation**:\n",
    "\n",
    "$$ XX^T \\beta = XY^T $$\n",
    "\n",
    "from which we get an expression for the parameters:\n",
    "\n",
    "$$ \\beta = (XX^T)^{-1} XY^T $$\n",
    "\n",
    "Shapes of each term are $p \\times p$, $p \\times n$, and $n \\times 1$ respectively. A potential problem is when the matrix $XX^T$ is close to being irregular (e.g. when there are too few points or too many dimensions). When the measurements are correlated, we get a single tiny eigenvalue, thus the inverse would not be well-defined. In particular, if the measurements lie on a horizontal line in the feature space, the fitted coefficients will have low uncertainty along that horizontal dimension but high uncertainty along the dimension with ill-defined slope (the vertical dimension).This well-posedness of the problem depends only on the explanatory variables but not the response!\n",
    "\n",
    "Additional comments:\n",
    "* The parameter estimate is a linear function of Y\n",
    "* Hence, the estimated parameters are also random variables.\n",
    "* Up to normalization, $(XX^T)^{-1}$ is the empirical estimate of the covariance.\n",
    "\n",
    "To get prediction, compute\n",
    "\n",
    "$$\\hat{Y} = \\beta^T X = Y X^T (XX^T)^{-1} X = Y H $$\n",
    "\n",
    ", where $H = X^T (XX^T)^{-1} X$ is often called the \"hat matrix\". For completeness, the residuals are\n",
    "\n",
    "$$\\epsilon = Y - \\hat{Y} = Y (I - H) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the learned coefficients\n",
    "\n",
    "So far, we have found an estimate for the mean of the distribution of the random variable $\\beta$. Next consider\n",
    "\n",
    "$$ \\begin{split}\n",
    "\\mathrm{Cov}_\\epsilon (\\hat{\\beta}) & = \\mathrm{Cov}_\\epsilon ((XX^T)^{-1} XY^T) \\\\\n",
    "    & = \\mathrm{Cov}_\\epsilon ((XX^T)^{-1} X(X^T \\beta^* + \\epsilon^T)) \n",
    " \\end{split}$$\n",
    " \n",
    "where $\\beta^*$ is the vector of true parameters. Now, note that the first term in the summation is deterministic because terms cancel, thus it can be omitted:\n",
    "\n",
    "$$ \\begin{split}\n",
    "\\mathrm{Cov}_\\epsilon (\\hat{\\beta}) & = \\mathrm{Cov}_\\epsilon ((XX^T)^{-1} X\\epsilon^T) \\\\\n",
    "    & = (XX^T)^{-1} X \\, \\mathrm{Cov} \\, (\\epsilon^T)X^T(XX^T)^{-1}\n",
    " \\end{split}$$\n",
    " \n",
    "where we have made use of $ \\mathrm{Cov}(A\\epsilon^T) = A \\mathrm{Cov}(\\epsilon^T) A^T$. Now, using the fact that $\\epsilon$ is iid so that $ \\mathrm{Cov} (\\epsilon) = \\sigma^2 I$, we get\n",
    "\n",
    "$$ \\mathrm{Cov}(\\hat{\\beta}) = \\sigma^2 (XX^T)^{-1} $$\n",
    "\n",
    "This result holds true for any distributions of residuals $\\epsilon$ so long as $\\epsilon$ is iid with $\\mathbb{E}(\\epsilon) = 0$. If in addition we assume $ \\epsilon ~ N(0,\\sigma^2)$ then $\\hat{\\beta}$ is also normally distributed. We assume here that the estimate is bias-free: the distributions are centered on the true parameters $\\beta^*$. One disadvantage of linear regression is that the estimate $\\hat{\\beta}$ is sensitive to outliers, e.g. a single outlier in the measurements of $Y$ can drastically change the parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
