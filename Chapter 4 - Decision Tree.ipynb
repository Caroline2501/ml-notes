{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Tree (CART)\n",
    "\n",
    "_Kevin Siswandi_  \n",
    "**Fundamentals of Machine Learning**  \n",
    "June 2020  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to construct a decision tree, but a standard choice is to consider one feature at a time. To select at which location a split should be made (and for which feature), a measure of impurity is needed to assess the quality of a split. Given the best first split (for a particular location for a particular feature), this will partition the feature space into two regions. Next, this process is repeated for each of the two regions to find the next best split, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Construction\n",
    "\n",
    "Therefore, the basic idea is to recursively split the feature space according to some binary criteria. Such a partitioning of the feature space can be represented as a decision tree, where the resulting regions due to the partitioning correspond to the leaf nodes of the tree. In other words, we apply a series of binary yes/no questions to\n",
    "+ iteratively partition the feature space at train time (i.e. build a decision tree)\n",
    "+ classify query at test time (by traversing the tree)\n",
    "\n",
    "Note that this process assumes that the training data is representative of what we want to predict (i.e. the test data should be from the same distribution as the training set). There are some arbitrary choices involved:\n",
    "+ greedy tree construction (because finding the optimal tree according to some criteria is NP-hard, i.e. impractical)\n",
    "+ binary decision tree based on binary splits (yes/no questions): each node has two children\n",
    "+ split into contiguous regions in feature space (i.e. only one parameter per node/split)\n",
    "+ single feature for each split (monothetic decision tree) as opposed to oblique decision tree (with multiple features each split)\n",
    "+ impurity criteria for evaluating splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity Criteria\n",
    "\n",
    "Several impurity criteria exist, the most popular ones are:\n",
    "1. misclassification error\n",
    "2. entropy, and\n",
    "3. Gini criteria.\n",
    "\n",
    "Gini is the most popular choice (and default in `scikit-learn`) for classification problem, while mean decrease accuracy is often used for regression problem. **The Gini criterion is the expected error if we randomly pick a sample with probability $p(y)$ from the node and give its label to the entire node**:\n",
    "\n",
    "$$ \\begin{split}\n",
    "\\sum_y p(y) \\left(\\sum_{\\tilde{y} \\neq y} p(\\tilde{y})\\right) & = \\sum_y p(y)(1-p(y))\\\\\n",
    " & = \\sum_y p(y) - \\sum_y p^2(y) \\\\\n",
    " & = 1 - \\sum_y p^2(y)\n",
    "\\end{split}$$\n",
    "\n",
    "Plotting it against $p(y)$, one gets a parabola for the Gini criteria (with peak at $p(y) = 0.5$, a triangle for the misclassification accuracy, and infinite slope at origin/unity for the entropy. For further details, see [The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman](https://web.stanford.edu/~hastie/ElemStatLearn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring split quality\n",
    "\n",
    "Priority for split candidates is measured by the possible reduction in (node-normalized) impurity:\n",
    "\n",
    "$$ H - H(L) \\frac{N_L}{N_L + N_R} - H(R) \\frac{N_R}{N_L + N_R} $$\n",
    "\n",
    "where\n",
    "- $N_L$ is the amount of samples that end up in the left node\n",
    "- $N_R$ is the amount of samples that end up in the right node.\n",
    "- $H$ is the impurity of the node under consideration\n",
    "- $H(R)$ is the impurity of the right child\n",
    "- $H(L)$ is the impurity of the left child\n",
    "\n",
    "In the absence of normalization, the resulting tree would be very deep (too many splits), with each region containing only a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
