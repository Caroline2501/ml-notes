{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We continue our discussion of linear regression in chapter 7.\n",
    "\n",
    "### Review of last chapter\n",
    "\n",
    "In practice, to get the estimated parameters $\\hat{\\beta}$, one solves the normal equation directly instead of taking the inverse of $(XX^T)$. Now, we say that the matrix is irregular if the inverse does not exist, and likewise we say that the matrix is regular if the inverse exists. Often, $XX^T$ is nearly irregular when working with real data. Possible reasons are:\n",
    "+ number of features are larger than number of samples (p > N)\n",
    "+ collinearity in the observations\n",
    "\n",
    "In particular, OLS has zero bias: as $N \\rightarrow \\infty$, we are going to recover the true parameter values. *But* OLS suffers from high variance.\n",
    "\n",
    "### Bias-variance decomposition of mean squared error\n",
    "\n",
    "Let's assume that the parameter is a scalar; the mean squared error of the estimate *w.r.t.* the true parameter is:\n",
    "\n",
    "$$ \\begin{split}\n",
    "\\mathbb{E}[(\\hat{\\theta} - \\theta)^2] & = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] + \\mathbb{E}[\\hat{\\theta}] - \\theta)^2] \\\\\n",
    "   & = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] + \\mathbb{E}[(\\mathbb{E}[\\hat{\\theta}] - \\theta)^2] + 2 \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])(\\mathbb{E}[\\hat{\\theta}] - \\theta)]\\\\\n",
    "   & = \\mathbb{E}[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2] + (\\mathbb{E}[\\hat{\\theta}] - \\theta)^2\n",
    "\\end{split}$$\n",
    "\n",
    "The first term is the **variance** (i.e. the mean squared deviation of the parameter estimate from the expected parameter estimate); the second term is the squared **bias** (i.e. how much the expected estimate deviates from the true parameter value).\n",
    "\n",
    "<img src=\"img/empirical-mse.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "General idea of regularization: reduce variance by limiting the flexibility of the model + choose a sweet spot by cross-validation. Three concrete methods will be discussed below.\n",
    "\n",
    "### PCR (Principal Component Regression)\n",
    "\n",
    "* project data to r-dim subspace (r < p)\n",
    "* then do OLS (ordinary least squares) in that subspace\n",
    "\n",
    "Here we keep only the first few eigenvalues (out of total p) of $XX^T$, as small eigenvalues give unstable results (high uncertainty in the corresponding direction of the eigenvector).\n",
    "\n",
    "### Ridge Regression (1970s)\n",
    "\n",
    "$$\\min_\\beta || Y - \\beta^T X||^2_2 + \\lambda ||\\beta||^2_2 $$\n",
    "\n",
    "The first term is the OLS, second term bias solution towards modest slopes. $\\lambda$ is the regularization strength. This can be solved in closed form by taking the partial derivative w.r.t. $\\beta$:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\beta} \\left( YY^T - 2 Y X^T \\beta + \\beta^T XX^T \\beta + \\lambda \\beta^T \\beta \\right) \\\\\n",
    "= -2 YX^T + 2 \\beta^T XX^T + 2 \\lambda \\beta^T$$\n",
    "\n",
    "Setting this expression to zero, we get\n",
    "\n",
    "$$ \\beta^T (XX^T + \\lambda I) = Y X^T $$\n",
    "\n",
    "or an explicit expression for the parameter estimate\n",
    "\n",
    "$$ \\hat{\\beta} = (XX^T + \\lambda I)^{-1} X Y^T $$\n",
    "\n",
    "Note the additional ridge regularization term (the identity matrix multiplied by a constant). While PCR completely ignores small eigenvalues, ridge stabilizes the inverse operation by increasing all eigenvalues of $XX^T$ by a constant value. In other words, ridge penalty artificially inflates the covariance matrix $XX^T$ in all dimensions, i.e. it spreads out the data in all dimensions. As one increases the regularization term, eventually all coefficients would become zero, but it can't make the problem sparser by selectively making certain coefficients to be close to zero (useful for feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO regression (~1996)\n",
    "\n",
    "Our aim is to be able to make the problem sparser (i.e. setting some coefficients to zero). We can introduce\n",
    "\n",
    "$$ \\min_\\beta || Y - \\beta^T X||^2_2 + \\lambda || \\beta||_0$$\n",
    "\n",
    "where the L0-norm counts the number of nonzero components in the vector. For instance, when $p = 2$, we have to compute three solutions:\n",
    "1. $\\beta_1 = 0, \\beta_2 \\neq 0$\n",
    "2. $\\beta_1 \\neq 0, \\beta_2 = 0$\n",
    "3. $\\beta_1 \\neq 0, \\beta_2 \\neq 0$\n",
    "\n",
    "Due to the combinatorial explosion of number of possible solutions to try, we need something more tractable than the L0-norm. Hence we introduce the $L_p$-norm (the $p$ here is not related to the dimension of the features, so we use $l$ instead in the formula below to avoid confusion):\n",
    "\n",
    "<img src=\"img/equicountour.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "The equicountour lines (the surfaces where the values of the loss are equal) for intermediate values of $l$ can be interpolated from the ones shown above. When $l \\geq 1$, however, the regularizers are convex. We therefore use the L1-norm for LASSO regression:\n",
    "\n",
    "$$ \\min_\\beta \\mathrm{SSQ}(\\beta | X, Y) = \\min_\\beta (Y - \\beta^T X) (Y - \\beta^T X)^T + \\lambda || \\beta||_1 $$\n",
    "\n",
    "We can also write the regularization term as follows\n",
    "\n",
    "$$ || \\beta||_1 = a(\\beta)^T \\beta$$\n",
    "\n",
    "where $a(\\beta) \\in \\{-1, +1\\}^p$ by the definition of the L1-norm. Here, it is clear that $||\\beta||_1$ is just a plane in each of the quadrants. Therefore, the SSQ is still a parabola because $ YY^T + \\beta^T XX^T \\beta - 2 \\beta^T XY^T$ is a parabola and $a(\\beta)^T \\beta$ are different planes in each orthant (=generalization of quadrant to higher dimensions). In other words, the objective is just a parabola in each orthant. Moreover, the parabolae match up at the orthant boundaries since the function $||\\beta||_1$ is continous across orthants. Thanks to convexity, there is a single minimum.\n",
    "\n",
    "<img src=\"img/lasso-energy.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Let's now compare the solutions of LASSO and ridge by using the equivalent constraint formulations to see why we get sparse solutions in the case of LASSO but not ridge.\n",
    "\n",
    "<img src=\"img/lasso-vs-ridge.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Moreover, the space of sparse solutions becomes larger as $\\kappa$ is reduced (or equivalently if $\\lambda$ is increased):\n",
    "\n",
    "<img src=\"img/space-sparse.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Now, as an example of linear regression, consider computed tomography:\n",
    "* measurements $y$ are the detected absorption with shape (1 x n), where n is the number of rays\n",
    "* unknowns $\\hat{\\beta}$ are the intensities of pixels with shape (p x 1), where p is the number of pixels\n",
    "\n",
    "<img src=\"img/tomography.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
