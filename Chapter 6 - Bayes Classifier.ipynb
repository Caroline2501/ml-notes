{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Classifier and Bayes Risk\n",
    "\n",
    "_Kevin Siswandi_  \n",
    "**Fundamentals of Machine Learning**  \n",
    "June 2020  \n",
    "\n",
    "In this chapter, we will discuss the best possible classifier in machine learning.\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "First let's review some basic concepts of probabilites. The product rule is\n",
    "\n",
    "$$ P(\\alpha \\cap \\beta) = P(\\alpha | \\beta) * P(\\beta) $$\n",
    "\n",
    "which is the foundation of Bayes Theorem:\n",
    "\n",
    "$$ P(\\alpha | \\beta) = \\frac{P(\\beta|\\alpha) P(\\alpha)}{P(\\beta)}$$\n",
    "\n",
    "This can be generalized to multiple events using the *chain rule of probability*:\n",
    "\n",
    "$$ P(\\alpha, \\beta, \\gamma) = P(\\alpha | \\beta, \\gamma) P(\\beta|\\gamma) P(\\gamma) = P(\\beta|\\alpha,\\gamma) P(\\alpha | \\gamma) P(\\gamma) = ...$$\n",
    "\n",
    "One consequence is that the expectation values also follow the rule\n",
    "\n",
    "$$ \\mathbb{E}_{\\alpha,\\beta}[.] = \\mathbb{E}_{\\alpha}\\mathbb{E}_{\\beta|\\alpha}[.] $$\n",
    "\n",
    "or in the case of independent sampling process:\n",
    "\n",
    "$$ \\mathbb{E}_{\\alpha,\\beta}[.] = \\mathbb{E}_{\\alpha}\\mathbb{E}_{\\beta}[.]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Distribution\n",
    "\n",
    "- Multivariate is just statistician's speak for \"multidimensional\"\n",
    "- Multivariate probability distribution, just like any other probability distribution, is normalized\n",
    "\n",
    "Say we have a discrete distribution of two random variables as shown below (which are obviously not independent)\n",
    "\n",
    "<img src=\"img/multivariate.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "The conditional probability $P(b|a = a_0)$ is conditioned on a specific value of $a = a_0$ and the function is really only dependent on $b$. The key thing to remember is that marginal distribution of a joint normal is normal, and the conditional distribution of a joint normal is normal. However, the joint distribution of normal marginal distributions is not necessarily normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical decision theory\n",
    "\n",
    "Assume that we have a binary classification problem with set of class labels $Y \\in \\{1, 2\\}$. The questions that we would like to answer are:\n",
    "1. What does the best conceivable classifier look like?\n",
    "2. How good is it?\n",
    "\n",
    "In a nutshell: **Bayes classifier** is the proxy for the best possible classifier (not necessarily perfect), and **Bayes risk** measures how good a Bayes classifier is. Bayes classifier is obtained by minimizing Bayes risk.\n",
    "\n",
    "Statistical model of classification gives a recipe for how to generate an observation $(x,y)$:\n",
    "\n",
    "<img src=\"img/stats-classification.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "In order for this task to make sense, the joint distribution of the features should be different for different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and minimizing risk\n",
    "\n",
    "First we define a loss function $L(y, \\hat{y})$, where $y$ is the true class and $\\hat{y}$ is the predicted class. Although some mistakes are costlier in real life, a simple loss is the 0-1 loss below:\n",
    "\n",
    "<img src=\"img/01loss.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Aim of classification: minimize the expected loss (i.e. **risk**) of the classifier $f$:\n",
    "\n",
    "$$ R(f) = \\mathbb{E}_X \\mathbb{E}_{Y|X} L(Y = y, f(X = x))$$\n",
    "\n",
    "where $Y$ and $X$ are random variables and $f$ is your deterministic classifier which could be kNN, RF, etc. By the definition of the expectation value, we can write (assuming continuous RV)\n",
    "\n",
    "$$ R(f) = \\int_X \\mathbb{E}_{Y|X} L(y, f(x)) p(x) dx $$\n",
    "\n",
    "Therefore, to minimize risk, we minimize $ \\mathbb{E}_{Y|X} L(y, f(x)) $ at every point $x$ in space. We can write this as a discrete summation over all possible class labels:\n",
    "\n",
    "<img src=\"img/bayes-risk.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Note that the second summation involves an indicator function that charges the loss only for the instances that are actually predicted by the classifier. Also, only wrong predictions incur loss. The final result is that, for the 0-1 loss, Bayes classifier is\n",
    "\n",
    "$$ \\arg \\min_f R(f) $$\n",
    "\n",
    "which is given by\n",
    "\n",
    "$$ f(x) = \\arg \\max_{z \\in Y} p(z |x) $$\n",
    "\n",
    "Comments:\n",
    "* Bayes classifier relies on the knowledge of the true density (of the classes and also the prior probability for each class).\n",
    "* Bayes classifier is the best possible classifier, but best is not necessarily perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes decision boundary\n",
    "\n",
    "Consider the following toy example. There are two classes: red (class 1) and blue (class 2) in two dimensional feature space. The observations are sampled from some underlying distribution. \n",
    "\n",
    "<img src=\"img/bayes-theorem.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "We know the true probability density $p(x)$ as well as the priors $p(1)$ and $p(2)$, where $p(x, 1) = p(x|1) * p(1)$. Knowing these, we can compute the class density. The posterior probability $p(*|x)$ can be computed in terms of the prior and class density normalized by the density:\n",
    "- Class density, e.g. $p(x|1)$, is also called likelihood.\n",
    "- Class probability, e.g. $p(1)$, is also called prior probability.\n",
    "- Probability density, $p(x)$, is also called evidence.\n",
    "\n",
    "In practice, all terms in the RHS can be estimated from the data, e.g. kernel density estimate on the red points to get $p(x|1)$. For the 0-1-loss discussed above, the rule of Bayes classifier is to predict the class with the largest posterior probability $p(.|x)$. The **Bayes decision boundary (in green)** is the set for which the posterior probability for class 1 equals the one for class 2, namely $p(1|x) = p(2|x) = 0.5$. However, note that even the Bayes classifier is not perfect, because there are observations which are misclassified.\n",
    "\n",
    "**The Bayes Classifier is the best possible classifier for a given distribution and loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
