{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classifiers\n",
    "\n",
    "_Kevin Siswandi_  \n",
    "**Fundamentals of Machine Learning**  \n",
    "June 2020\n",
    "\n",
    "In a generative method, we estimate the class density, for each class, with a single Gaussian. Examples are Quadratic Discriminant Analysis and Linear Discriminant Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative vs Discriminative\n",
    "\n",
    "In machine learning, one typically makes distinction between generative vs discriminative classifiers:\n",
    "- Generative classifiers estimate class density, prior, and evidence (i.e. terms on the RHS). Example is Quadratic Discriminant Analysis.\n",
    "- Discriminative classifiers directly estimate posterior probability. Examples are k-NN, decision tree, random forest, etc.\n",
    "\n",
    "Although kernel density estimate can be used to approximate the class density, the data might have too few points and too many dimensions that make such kernel density estimate unreliable. In **quadratic discriminant analysis (QDA)**, we do the following:\n",
    "- approximate each class in the training data by a single Gaussian (estimate the mean and covariance matrix).\n",
    "- Then we apply Bayes theorem to compute the posterior probability.\n",
    "\n",
    "Note that we also need to estimate the prior (can be easily done by counting # of classes). The shape of the decision boundary can be found via Bayes Theorem as follows.\n",
    "\n",
    "$$\\begin{align*} p(1|x) & = p(2|x) \\\\\n",
    "    \\frac{p(x|1) p(1)}{p(x)} & = \\frac{p(x|2) p(2)}{p(x)} \\\\\n",
    "    c_1 \\exp[\\frac{-1}{2} (x - \\mu_1)^T \\Sigma_1^{-1} (x - \\mu_1)] p(1) & = c_2 \\exp[\\frac{-1}{2}(x - \\mu_2)^T \\Sigma_2^{-1} (x - \\mu_2)] p(2) \\\\\n",
    "    \\ln\\frac{c_1 p_1}{c_2 p_2} - \\frac{1}{2} (x - \\mu_1)^T \\Sigma_1^{-1} (x - \\mu_1) + \\frac{1}{2} (x - \\mu_2)^T \\Sigma_2^{-1} (x - \\mu_2) & = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now, the first term is a scalar constant, and therefore the equation is quadratic in $x$. This suggests that the decision boundaries of QDA are **quadrics**, which is an umbrella term for ellipse, hyperbola, parabola, and parallel lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Normal\n",
    "\n",
    "A multivariate Gaussian has two parameters: mean and covariance, where the covariance is defined as\n",
    "\n",
    "$$ \\begin{split}\n",
    "Cov(X) & := \\mathbb{E}[(X - \\mathbb{E}[X]) \\, (X - \\mathbb{E}[X])^T] \\\\\n",
    "    & = \\mathbb{E}[XX^T - X \\mathbb{E}[X^T] - E[X] X^T + E[X] E[X^T]] \\\\\n",
    "    & = \\mathbb{E}[XX^T] - \\mathbb{E}[X] \\mathbb{E}[X^T]\n",
    "    \\end{split}\n",
    "$$\n",
    "\n",
    "- $X$ is a vector $p \\times 1$ of random variables\n",
    "- $\\mathbb{E}[\\mathbb{E}[X]] = \\mathbb{E}[X]$ because expectation value of a random variable is a constant, and expectation value of a constant is the constant itself.\n",
    "- It follows that covariance of an affinely-transposed random variable is $Cov(AX) = A Cov(X) A^T$ for some matrix A.\n",
    "- The covariance matrix has dimension $p \\times p$.\n",
    "\n",
    "Empirical estimate of covariance is given by\n",
    "\n",
    "$$ \\frac{1}{n-1} S = \\frac{1}{n-1}XCC^T X^T = \\frac{1}{n-1}XCX^T$$\n",
    "\n",
    "where S is the scatter matrix and C is the centering matrixï¼Œwith $CC^T = C$. Once the data is centered once, nothing happens if we center it again $C = I - \\frac{1}{n} 1 * 1^T$; here $1$ is the column vector of ones. The isocountours of a multivariate Gaussian are shown below for negative cross-correlation (left), stronger negative cross-correlation, and positive cross-correlation with higher spread at the 2nd dimension.\n",
    "\n",
    "<img src=\"img/covariance.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "The diagonal elements has the variance of each dimension. The off-diagonal elements indicate how much linear association is between random variable along different dimensions. The **normal distribution** is defined as\n",
    "\n",
    "$$p(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp[-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)]$$\n",
    "\n",
    "Points worth noting:\n",
    "- The inverse covariance matrix $\\Sigma^{-1}$ is also called the precision matrix: the larger the covariance/spread, the \"better\" the precision is.\n",
    "- $\\mu$ is the mean vector of the data\n",
    "- marginal of a normal distribution is normal\n",
    "- conditional of a normal distribution is normal\n",
    "- normal marginal doesn't necessarily give normal joint distribution\n",
    "\n",
    "See https://juanitorduz.github.io/multivariate_normal/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
